{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование возможности последовательной загрузки датасета!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип датасета: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Количество примеров в датасете: 39780811\n"
     ]
    }
   ],
   "source": [
    "triplet_dataset = load_from_disk(\"msmarco-ru/triplets\")\n",
    "print(\"Тип датасета:\", type(triplet_dataset))\n",
    "print(\"Количество примеров в датасете:\", len(triplet_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'можно ли немного кофеина во время беременности',\n",
       " 'positive': 'Мы мало что знаем о влиянии кофеина во время беременности на вас и вашего ребенка. Так что лучше всего ограничить сумму, которую вы получаете каждый день. Если вы беременны, ограничьте количество кофеина 200 миллиграммами каждый день. Это примерно столько, сколько содержится в чашке кофе объемом 1–8 унций или одной чашке кофе объемом 12 унций.',\n",
       " 'negative': 'Как правило, беременным женщинам безопасно есть шоколад, потому что исследования доказали определенные преимущества употребления шоколада во время беременности. Однако беременным женщинам следует следить за тем, чтобы потребление кофеина не превышало 200 мг в день.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_from_disk() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsmarco-ru/collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_from_disk() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_from_disk(\"msmarco-ru/collection\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 363550,\n",
       " 'text': 'Школьные округа округа Пассаик ‚Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚ Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚ â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â ¦ÃƒÂ ¢ Ã‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦..ÃƒÂ ¢ Ã ‚Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚ Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚ â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â Â ¢ Ã‚â‚¬Ã‚Â¦ 13. BloomingdaleÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ Ã ‚Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚ Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ ............ 13 ПК Технический институт ‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ 25. Клифтон. Ã‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã ‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ Ã ‚Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚ Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ 14 Passaic Valley Reg. H.S. Район # 1ÃƒÂ ‚¬Ã‚Â¦ÃƒÂ ¢ ‚Â‚¬ÃÂ¦ 26. ХаледонÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ Ã‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã ‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ Ã ‚Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚ Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦.ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ 16 PatersonÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ Ã‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã ‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ Ã ‚Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚ Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ 27. HawthorneÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ Ã‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã ‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ Ã Â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ÃƒÂ ¢ ‚â‚¬Ã‚Â¦ ..'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[363550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DictDatasetWrapper:\n",
    "    def __init__(self,data_dir:str):\n",
    "        self.dataset = load_from_disk(data_dir)\n",
    "        self.dataset_iterable = None \n",
    "\n",
    "    def __iter__(self):\n",
    "        self.dataset_iterable = iter(self.dataset[\"train\"])\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        new_line = next(self.dataset_iterable)\n",
    "        return (new_line[\"query\"].strip(),new_line[\"positive\"].strip(),new_line[\"negative\"].strip())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "import os\n",
    "\n",
    "class PairsDatasetPreLoad(IterableDataset):\n",
    "    \"\"\"\n",
    "    dataset to iterate over a collection of pairs, format per line: q \\t d_pos \\t d_neg\n",
    "    we preload everything in memory at init\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        super(PairsDatasetPreLoad, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.id_style = \"row_id\"\n",
    "        \n",
    "        self.dataset = DictDatasetWrapper(data_dir)\n",
    "\n",
    "        # количество экземпляров    \n",
    "        self.nb_ex = len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    # возврат количества экземплятров в файле\n",
    "    def __len__(self):\n",
    "        return self.nb_ex\n",
    "\n",
    "    # нет такого в iterableDataset\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.data_dict[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PairsDatasetPreLoad(\"msmarco-ru/triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split \n",
    "\n",
    "# проверка на возможность разбиения датасета! \n",
    "train_data,val_data = random_split(dataset, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def rename_keys(d, prefix):\n",
    "    return {prefix + \"_\" + k: v for k, v in d.items()}\n",
    "\n",
    "class DataLoaderWrapper(DataLoader):\n",
    "    def __init__(self, tokenizer_type, max_length, **kwargs):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
    "        super().__init__(collate_fn=self.collate_fn, **kwargs, pin_memory=True)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        raise NotImplementedError(\"must implement this method\")\n",
    "\n",
    "\n",
    "class SiamesePairsDataLoader(DataLoaderWrapper):\n",
    "    \"\"\"Siamese encoding (query and document independent)\n",
    "    train mode (triplets)\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        batch is a list of tuples, each tuple has 3 (text) items (q, d_pos, d_neg)\n",
    "        \"\"\"\n",
    "        #q - кортеж запросов \n",
    "        #d_pos - кортеж d_pos\n",
    "        #d_neg - кортеж d_neg\n",
    "        q, d_pos, d_neg = zip(*batch)\n",
    "        # обработка + обрезка + padding (нужен для берта!)\n",
    "        q = self.tokenizer(list(q),\n",
    "                           add_special_tokens=True,\n",
    "                           padding=\"longest\",  # pad to max sequence length in batch\n",
    "                           truncation=\"longest_first\",  # truncates to self.max_length\n",
    "                           max_length=self.max_length,\n",
    "                           return_attention_mask=True)\n",
    "        \n",
    "        d_pos = self.tokenizer(list(d_pos),\n",
    "                               add_special_tokens=True,\n",
    "                               padding=\"longest\",  # pad to max sequence length in batch\n",
    "                               truncation=\"longest_first\",  # truncates to self.max_length\n",
    "                               max_length=self.max_length,\n",
    "                               return_attention_mask=True)\n",
    "        \n",
    "        d_neg = self.tokenizer(list(d_neg),\n",
    "                               add_special_tokens=True,\n",
    "                               padding=\"longest\",  # pad to max sequence length in batch\n",
    "                               truncation=\"longest_first\",  # truncates to self.max_length\n",
    "                               max_length=self.max_length,\n",
    "                               return_attention_mask=True)\n",
    "        \n",
    "        # переименование ключей (добавление префикса ко всему, что возвращает токенизатор)\n",
    "        sample = {**rename_keys(q, \"q\"), **rename_keys(d_pos, \"pos\"), **rename_keys(d_neg, \"neg\")} # множество словарей\n",
    "        return {k: torch.tensor(v) for k, v in sample.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = SiamesePairsDataLoader(dataset=val_data, batch_size=16,\n",
    "                                                     shuffle=False,\n",
    "                                                     #! num_workers must be 1!\n",
    "                                                     num_workers=1,\n",
    "                                                     tokenizer_type=\"ai-forever/ruBert-base\",\n",
    "                                                     max_length=256, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = SiamesePairsDataLoader(dataset=train_data, batch_size=16,shuffle=False,\n",
    "                                                     #! num_workers must be 1!\n",
    "                                                     num_workers=1,\n",
    "                                                     tokenizer_type=\"ai-forever/ruBert-base\",\n",
    "                                                     max_length=256, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Caught NotImplementedError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 63, in __getitem__\n    raise NotImplementedError(\"Subclasses of Dataset should implement __getitem__.\")\nNotImplementedError: Subclasses of Dataset should implement __getitem__.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(val_dataloader_iter))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Caught NotImplementedError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/lesha/mamba/envs/splade-v2/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 63, in __getitem__\n    raise NotImplementedError(\"Subclasses of Dataset should implement __getitem__.\")\nNotImplementedError: Subclasses of Dataset should implement __getitem__.\n"
     ]
    }
   ],
   "source": [
    "val_dataloader_iter = iter(val_dataloader)\n",
    "train_dataloader_iter = iter(train_dataloader)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(next(train_dataloader_iter))\n",
    "    print(next(val_dataloader_iter))\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
